{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Chat_Analytics.ipynb",
      "provenance": [],
      "mount_file_id": "1kOlqMobZTsszKfCR5IzxsmxuTwZQNSxJ",
      "authorship_tag": "ABX9TyPsoIJGadF3S2YWBK/rzBai",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/swahareddy/WhatsappAnalytics/blob/master/Chat_Analytics.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RdTNrR6Z4uKw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "5bb1b28e-27d9-4ff4-c1a0-a701318a8041"
      },
      "source": [
        "file=open('/content/drive/My Drive/Colab Notebooks/WhatsApp Chat with Humanity Debroy.txt', mode='r', encoding='utf-8')\n",
        "chat_history=file.read()[:10]\n",
        "print(chat_history)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "11/12/18, \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "n0LmuZR-BG-o",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Setting up connection with GCP\n",
        "import os\n",
        "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = \"project_credentials.json\"\n",
        "project_id=\"maybe-well-do-something\"\n",
        "\n",
        "from google.cloud import language_v1\n",
        "from google.cloud.language_v1 import enums\n",
        "\n",
        "def sample_analyze_sentiment(text_content):\n",
        "    client = language_v1.LanguageServiceClient()\n",
        "    # Available types: PLAIN_TEXT, HTML\n",
        "    type_ = enums.Document.Type.PLAIN_TEXT\n",
        "\n",
        "    document = {\"content\": text_content, \"type\": type_}\n",
        "    encoding_type = enums.EncodingType.UTF8 # Available values: NONE, UTF8, UTF16, UTF32\n",
        "    try:\n",
        "      response = client.analyze_sentiment(document, encoding_type=encoding_type)\n",
        "      # print(u\"Language of the text: {}\".format(response.language))\n",
        "      sentiment_output={'Sentiment_score':response.document_sentiment.score,\n",
        "                        'Sentiment_mag':response.document_sentiment.magnitude,\n",
        "                        'valid_language':response.language}\n",
        "      # print (sentiment_output)\n",
        "    except:\n",
        "      sentiment_output={'Sentiment_score':0,\n",
        "                        'Sentiment_mag':0,\n",
        "                        'valid_language':0}\n",
        "    # print(u\"Language of the text: {}\".format(response.language))\n",
        "    return sentiment_output\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "moZBXSQDEmHi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Deal with emoticons source: https://en.wikipedia.org/wiki/List_of_emoticons\n",
        "def load_dict_smileys():\n",
        "    \n",
        "    return {\n",
        "        \":‑)\":\"smiley\",\n",
        "        \":-]\":\"smiley\",\n",
        "        \":-3\":\"smiley\",\n",
        "        \":->\":\"smiley\",\n",
        "        \"8-)\":\"smiley\",\n",
        "        \":-}\":\"smiley\",\n",
        "        \":)\":\"smiley\",\n",
        "        \":]\":\"smiley\",\n",
        "        \":3\":\"smiley\",\n",
        "        \":>\":\"smiley\",\n",
        "        \"8)\":\"smiley\",\n",
        "        \":}\":\"smiley\",\n",
        "        \":o)\":\"smiley\",\n",
        "        \":c)\":\"smiley\",\n",
        "        \":^)\":\"smiley\",\n",
        "        \"=]\":\"smiley\",\n",
        "        \"=)\":\"smiley\",\n",
        "        \":-))\":\"smiley\",\n",
        "        \":‑D\":\"smiley\",\n",
        "        \"8‑D\":\"smiley\",\n",
        "        \"x‑D\":\"smiley\",\n",
        "        \"X‑D\":\"smiley\",\n",
        "        \":D\":\"smiley\",\n",
        "        \"8D\":\"smiley\",\n",
        "        \"xD\":\"smiley\",\n",
        "        \"XD\":\"smiley\",\n",
        "        \":‑(\":\"sad\",\n",
        "        \":‑c\":\"sad\",\n",
        "        \":‑<\":\"sad\",\n",
        "        \":‑[\":\"sad\",\n",
        "        \":(\":\"sad\",\n",
        "        \":c\":\"sad\",\n",
        "        \":<\":\"sad\",\n",
        "        \":[\":\"sad\",\n",
        "        \":-||\":\"sad\",\n",
        "        \">:[\":\"sad\",\n",
        "        \":{\":\"sad\",\n",
        "        \":@\":\"sad\",\n",
        "        \">:(\":\"sad\",\n",
        "        \":'‑(\":\"sad\",\n",
        "        \":'(\":\"sad\",\n",
        "        \":‑P\":\"playful\",\n",
        "        \"X‑P\":\"playful\",\n",
        "        \"x‑p\":\"playful\",\n",
        "        \":‑p\":\"playful\",\n",
        "        \":‑Þ\":\"playful\",\n",
        "        \":‑þ\":\"playful\",\n",
        "        \":‑b\":\"playful\",\n",
        "        \":P\":\"playful\",\n",
        "        \"XP\":\"playful\",\n",
        "        \"xp\":\"playful\",\n",
        "        \":p\":\"playful\",\n",
        "        \":Þ\":\"playful\",\n",
        "        \":þ\":\"playful\",\n",
        "        \":b\":\"playful\",\n",
        "        \"<3\":\"love\"\n",
        "        }\n",
        "  \n",
        "#CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
        "def load_dict_contractions():\n",
        "    \n",
        "    return {\n",
        "        \"ain't\":\"is not\",\n",
        "        \"amn't\":\"am not\",\n",
        "        \"aren't\":\"are not\",\n",
        "        \"can't\":\"cannot\",\n",
        "        \"'cause\":\"because\",\n",
        "        \"couldn't\":\"could not\",\n",
        "        \"couldn't've\":\"could not have\",\n",
        "        \"could've\":\"could have\",\n",
        "        \"daren't\":\"dare not\",\n",
        "        \"daresn't\":\"dare not\",\n",
        "        \"dasn't\":\"dare not\",\n",
        "        \"didn't\":\"did not\",\n",
        "        \"doesn't\":\"does not\",\n",
        "        \"don't\":\"do not\",\n",
        "        \"e'er\":\"ever\",\n",
        "        \"em\":\"them\",\n",
        "        \"everyone's\":\"everyone is\",\n",
        "        \"finna\":\"fixing to\",\n",
        "        \"gimme\":\"give me\",\n",
        "        \"gonna\":\"going to\",\n",
        "        \"gon't\":\"go not\",\n",
        "        \"gotta\":\"got to\",\n",
        "        \"hadn't\":\"had not\",\n",
        "        \"hasn't\":\"has not\",\n",
        "        \"haven't\":\"have not\",\n",
        "        \"he'd\":\"he would\",\n",
        "        \"he'll\":\"he will\",\n",
        "        \"he's\":\"he is\",\n",
        "        \"he've\":\"he have\",\n",
        "        \"how'd\":\"how would\",\n",
        "        \"how'll\":\"how will\",\n",
        "        \"how're\":\"how are\",\n",
        "        \"how's\":\"how is\",\n",
        "        \"I'd\":\"I would\",\n",
        "        \"I'll\":\"I will\",\n",
        "        \"I'm\":\"I am\",\n",
        "        \"I'm'a\":\"I am about to\",\n",
        "        \"I'm'o\":\"I am going to\",\n",
        "        \"isn't\":\"is not\",\n",
        "        \"it'd\":\"it would\",\n",
        "        \"it'll\":\"it will\",\n",
        "        \"it's\":\"it is\",\n",
        "        \"I've\":\"I have\",\n",
        "        \"kinda\":\"kind of\",\n",
        "        \"let's\":\"let us\",\n",
        "        \"mayn't\":\"may not\",\n",
        "        \"may've\":\"may have\",\n",
        "        \"mightn't\":\"might not\",\n",
        "        \"might've\":\"might have\",\n",
        "        \"mustn't\":\"must not\",\n",
        "        \"mustn't've\":\"must not have\",\n",
        "        \"must've\":\"must have\",\n",
        "        \"needn't\":\"need not\",\n",
        "        \"ne'er\":\"never\",\n",
        "        \"o'\":\"of\",\n",
        "        \"o'er\":\"over\",\n",
        "        \"ol'\":\"old\",\n",
        "        \"oughtn't\":\"ought not\",\n",
        "        \"shalln't\":\"shall not\",\n",
        "        \"shan't\":\"shall not\",\n",
        "        \"she'd\":\"she would\",\n",
        "        \"she'll\":\"she will\",\n",
        "        \"she's\":\"she is\",\n",
        "        \"shouldn't\":\"should not\",\n",
        "        \"shouldn't've\":\"should not have\",\n",
        "        \"should've\":\"should have\",\n",
        "        \"somebody's\":\"somebody is\",\n",
        "        \"someone's\":\"someone is\",\n",
        "        \"something's\":\"something is\",\n",
        "        \"that'd\":\"that would\",\n",
        "        \"that'll\":\"that will\",\n",
        "        \"that're\":\"that are\",\n",
        "        \"that's\":\"that is\",\n",
        "        \"there'd\":\"there would\",\n",
        "        \"there'll\":\"there will\",\n",
        "        \"there're\":\"there are\",\n",
        "        \"there's\":\"there is\",\n",
        "        \"these're\":\"these are\",\n",
        "        \"they'd\":\"they would\",\n",
        "        \"they'll\":\"they will\",\n",
        "        \"they're\":\"they are\",\n",
        "        \"they've\":\"they have\",\n",
        "        \"this's\":\"this is\",\n",
        "        \"those're\":\"those are\",\n",
        "        \"'tis\":\"it is\",\n",
        "        \"'twas\":\"it was\",\n",
        "        \"wanna\":\"want to\",\n",
        "        \"wasn't\":\"was not\",\n",
        "        \"we'd\":\"we would\",\n",
        "        \"we'd've\":\"we would have\",\n",
        "        \"we'll\":\"we will\",\n",
        "        \"we're\":\"we are\",\n",
        "        \"weren't\":\"were not\",\n",
        "        \"we've\":\"we have\",\n",
        "        \"what'd\":\"what did\",\n",
        "        \"what'll\":\"what will\",\n",
        "        \"what're\":\"what are\",\n",
        "        \"what's\":\"what is\",\n",
        "        \"what've\":\"what have\",\n",
        "        \"when's\":\"when is\",\n",
        "        \"where'd\":\"where did\",\n",
        "        \"where're\":\"where are\",\n",
        "        \"where's\":\"where is\",\n",
        "        \"where've\":\"where have\",\n",
        "        \"which's\":\"which is\",\n",
        "        \"who'd\":\"who would\",\n",
        "        \"who'd've\":\"who would have\",\n",
        "        \"who'll\":\"who will\",\n",
        "        \"who're\":\"who are\",\n",
        "        \"who's\":\"who is\",\n",
        "        \"who've\":\"who have\",\n",
        "        \"why'd\":\"why did\",\n",
        "        \"why're\":\"why are\",\n",
        "        \"why's\":\"why is\",\n",
        "        \"won't\":\"will not\",\n",
        "        \"wouldn't\":\"would not\",\n",
        "        \"would've\":\"would have\",\n",
        "        \"y'all\":\"you all\",\n",
        "        \"you'd\":\"you would\",\n",
        "        \"you'll\":\"you will\",\n",
        "        \"you're\":\"you are\",\n",
        "        \"you've\":\"you have\",\n",
        "        \"Whatcha\":\"What are you\",\n",
        "        \"luv\":\"love\",\n",
        "        \"sux\":\"sucks\"\n",
        "        }"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ru6AeBbGZ8J0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "def clean_urls(message):\n",
        "  regex = r\"(?i)\\b((?:https?://|www\\d{0,3}[.]|[a-z0-9.\\-]+[.][a-z]{2,4}/)(?:[^\\s()<>]+|\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\))+(?:\\(([^\\s()<>]+|(\\([^\\s()<>]+\\)))*\\)|[^\\s`!()\\[\\]{};:'\\\".,<>?«»“”‘’]))\"\n",
        "  urls=re.findall(regex, message)\n",
        "  links=[]\n",
        "  for url in urls:\n",
        "    message=message.replace(url[0],\"\")\n",
        "    links.append(url[0])\n",
        "  return (message, links)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B-T1YyqaEqYp",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# !pip install bs4\n",
        "# !pip install emoji\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "import itertools\n",
        "import emoji\n",
        "\n",
        "def extract_emojis(s):\n",
        "  return ''.join(c for c in s if c in emoji.UNICODE_EMOJI)\n",
        "\n",
        "def message_cleaning_for_sentiment_analysis(message):    \n",
        "    #Inspired from https://towardsdatascience.com/twitter-sentiment-analysis-using-fasttext-9ccd04465597?gi=88f51bcffb06\n",
        "    #Escaping HTML characters\n",
        "    message = BeautifulSoup(message).get_text()\n",
        "   \n",
        "    #Special case not handled previously.\n",
        "    message = message.replace('\\x92',\"'\")\n",
        "    \n",
        "    #Capture and remove tags\n",
        "    mentioned=re.findall(\"(@[A-Za-z0-9]+)\", message)\n",
        "    message = ' '.join(re.sub(\"(@[A-Za-z0-9]+)\", \" \", message).split())\n",
        "\n",
        "    #Removal of address\n",
        "    message, urls = clean_urls(message)\n",
        "    \n",
        "    #Removal of Punctuation\n",
        "    #message = ' '.join(re.sub(\"[\\.\\,\\!\\?\\:\\;\\-\\=]\", \" \", message).split())\n",
        "    \n",
        "    #Lower case\n",
        "    #message = message.lower()\n",
        "    \n",
        "    #CONTRACTIONS source: https://en.wikipedia.org/wiki/Contraction_%28grammar%29\n",
        "    CONTRACTIONS = load_dict_contractions()\n",
        "    message = message.replace(\"’\",\"'\")\n",
        "    words = message.split()\n",
        "    reformed = [CONTRACTIONS[word] if word in CONTRACTIONS else word for word in words]\n",
        "    message = \" \".join(reformed)\n",
        "    \n",
        "    # Standardizing words\n",
        "    message = ''.join(''.join(s)[:2] for _, s in itertools.groupby(message))\n",
        "    \n",
        "    #Deal with emoticons source: https://en.wikipedia.org/wiki/List_of_emoticons\n",
        "    # Can consider using this too https://pypi.org/project/emosent-py/\n",
        "    SMILEY = load_dict_smileys()  \n",
        "    words = message.split()\n",
        "    reformed = [SMILEY[word] if word in SMILEY else word for word in words]\n",
        "    message = \" \".join(reformed)\n",
        "    \n",
        "    #Deal with emojis\n",
        "    message = emoji.demojize(message)\n",
        "\n",
        "    message = message.replace(\":\",\" \")\n",
        "    message = ' '.join(message.split())\n",
        "\n",
        "    return message, mentioned, urls\n",
        "\n",
        "# message=\"Hey there @Devarghya, did you check out the debate @949825774 on the #MeToo campaign at www.facebook.com/wtf and https://www.twitter.com/wth or even google.com/idk ?\"\n",
        "# print(message_cleaning_for_sentiment_analysis(message))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zA4qGf2NmhpD",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "per_person_info={\n",
        "    'Message level info':[],\n",
        "    'Messages count':0,\n",
        "    'Overall char count':0,\n",
        "    'Total sentiment':0\n",
        "}"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2tNBz_eZm1qE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import re\n",
        "language_list=[]\n",
        "member_score_dict={}\n",
        "member_msg_count_dict={}\n",
        "master_json={}\n",
        "timestamp_regex=\"[0-9][0-9]/[0-9][0-9]/[0-9][0-9], [0-9][0-9]:[0-9][0-9] - \"\n",
        "end_index=0\n",
        "trunc_chat_hist=chat_history\n",
        "timestamp_next=re.search(timestamp_regex, chat_history)\n",
        "print(timestamp_next)\n",
        "loop_count=0\n",
        "while end_index<len(trunc_chat_hist):\n",
        "\n",
        "  trunc_chat_hist=trunc_chat_hist[end_index:]\n",
        "  index=re.search(timestamp_regex, trunc_chat_hist)\n",
        "  if index==None:\n",
        "    print(trunc_chat_hist)\n",
        "    break\n",
        "  start_index=index.start()\n",
        "  message=trunc_chat_hist[:start_index]\n",
        "  end_index=index.end()\n",
        "  timestamp=timestamp_next\n",
        "  timestamp_next=trunc_chat_hist[start_index:end_index][:-3]\n",
        "  print(\"timestamp for next message is \"+timestamp_next)\n",
        "  message=message.replace(\"\\n\",\" \")\n",
        "  author_index=message.find(':')\n",
        "  author=message[:author_index]\n",
        "  # print(\"Author is - \"+author)\n",
        "  message=message[author_index+2:]\n",
        "  print(\"Message is - \"+message)\n",
        "  message=message.strip()\n",
        "\n",
        "  if (message==\"<Media omitted>\") or (message==\"This message was deleted\") or (\"the group description\" in message) or (\"this group's icon\" in message):\n",
        "    print (\"Exception detected\")\n",
        "    if author_index==-1:\n",
        "      print(trunc_chat_hist[:start_index].replace(\"\\n\",\" \"))\n",
        "    else:\n",
        "      print(message)\n",
        "    pass\n",
        "\n",
        "  else:\n",
        "    message_cleaned, tagged, urls=message_cleaning_for_sentiment_analysis(message)\n",
        "    sentiment_output=sample_analyze_sentiment(message)\n",
        "    # print(sentiment_output)\n",
        "    if author not in master_json.keys():\n",
        "      # print(type(author))\n",
        "      # print(author)\n",
        "      master_json[author]={}\n",
        "      master_json[author]['Message level info']=[]\n",
        "      master_json[author]['Overall char count']=0\n",
        "      master_json[author]['Messages count']=0\n",
        "      master_json[author]['Total sentiment score']=0.0\n",
        "      master_json[author]['Total sentiment magnitude']=0.0\n",
        "    \n",
        "    per_message_info={\n",
        "      'Author':author,\n",
        "      'Original message':message,\n",
        "      'Original message length':len(message),\n",
        "      'Sentiment score':sentiment_output['Sentiment_score'],\n",
        "      'Sentiment magnitude':sentiment_output['Sentiment_mag'],\n",
        "      'URLs':str(urls),\n",
        "      'Tagged':str(tagged),\n",
        "      'Time':str(timestamp).replace(',',''),\n",
        "      'Cleaned message':message_cleaned if message_cleaned!=message else 'No',\n",
        "      'Emojis':extract_emojis(message)\n",
        "    }\n",
        "\n",
        "    master_json[author]['Message level info'].append(per_message_info)\n",
        "    master_json[author]['Overall char count']=master_json[author]['Overall char count']+per_message_info['Original message length']\n",
        "    master_json[author]['Messages count']=master_json[author]['Messages count']+1\n",
        "    master_json[author]['Total sentiment score']=master_json[author]['Total sentiment score']+per_message_info['Sentiment score']\n",
        "    master_json[author]['Total sentiment magnitude']=master_json[author]['Total sentiment magnitude']+per_message_info['Sentiment magnitude']\n",
        "    # print(per_message_info)\n",
        "    loop_count=loop_count+1\n",
        "    print(loop_count)\n",
        "    # print(\"--------\")\n",
        "\n",
        "del(master_json[''])\n",
        "print(master_json.keys())\n",
        "print(master_json)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YgVPyNM33x8a",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 462
        },
        "outputId": "bdd815d9-4a46-4425-f1b1-72563a0bf4f0"
      },
      "source": [
        "import pandas as pd\n",
        "# author='Dharma'\n",
        "# print(list2)\n",
        "list_columns=master_json['Dharma']['Message level info'][0].keys()\n",
        "print(list_columns)\n",
        "df_full = pd.DataFrame(columns=list_columns)\n",
        "print(df_full)\n",
        "for author in master_json.keys():\n",
        "  for per_message_info in master_json[author]['Message level info']:\n",
        "    message_details_list=[]\n",
        "    for key in per_message_info.keys():\n",
        "      message_details_list.append(per_message_info[key])\n",
        "    # print([author]+message_details_list)\n",
        "    df_row=pd.DataFrame([message_details_list], columns=list_columns)\n",
        "    # print(df_row)\n",
        "    df_full=pd.concat([df_full,df_row], ignore_index=True)\n",
        "print(df_full)\n",
        "print(df_full.columns)\n",
        "df_full.to_csv (r'humanity_dataframe.csv')\n",
        "    "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_keys(['Author', 'Original message', 'Original message length', 'Sentiment score', 'Sentiment magnitude', 'URLs', 'Tagged', 'Time', 'Cleaned message', 'Emojis'])\n",
            "Empty DataFrame\n",
            "Columns: [Author, Original message, Original message length, Sentiment score, Sentiment magnitude, URLs, Tagged, Time, Cleaned message, Emojis]\n",
            "Index: []\n",
            "                 Author  ... Emojis\n",
            "0   Aditya Daftari Coep  ...       \n",
            "1   Aditya Daftari Coep  ...       \n",
            "2   Aditya Daftari Coep  ...       \n",
            "3   Aditya Daftari Coep  ...       \n",
            "4   Aditya Daftari Coep  ...       \n",
            "5   Aditya Daftari Coep  ...       \n",
            "6   Aditya Daftari Coep  ...      😂\n",
            "7   Abhishree Karmalkar  ...       \n",
            "8                Dharma  ...       \n",
            "9                Dharma  ...       \n",
            "10               Dharma  ...       \n",
            "11              Sherwin  ...     😂😂\n",
            "12              Sherwin  ...       \n",
            "13              Prabjot  ...       \n",
            "\n",
            "[14 rows x 10 columns]\n",
            "Index(['Author', 'Original message', 'Original message length',\n",
            "       'Sentiment score', 'Sentiment magnitude', 'URLs', 'Tagged', 'Time',\n",
            "       'Cleaned message', 'Emojis'],\n",
            "      dtype='object')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EkEsmXR3LD2j",
        "colab_type": "text"
      },
      "source": [
        "* Remove deletions\n",
        "* Remove images (what if image has a text with it?)\n",
        "* GIFs, images <Media omitted> looses txt alomng with info\n",
        "* does not keep reply context\n",
        "* Changed group icon\n",
        "* Changed group description\n",
        "* This message was deleted\n",
        "* Discard URLs\n",
        "* Do emojis help senti detection? - NO [link](https://https://pypi.org/project/emosent-py/) Do their text eqivalents help?\n",
        "\n",
        "* capture emojis\n",
        "* you deleted group descripyion. How to map you to Tejas\n",
        "hinglish tranlation"
      ]
    }
  ]
}